{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipelines for GoFactoring receivables status prediction\n",
    "# uses transactions aggregated into instruments and pre-processing from GF_analysis4.ipynb (@@to change to .py module)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn_pandas import DataFrameMapper, gen_features\n",
    "import itertools\n",
    "import sys\n",
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from work\n",
    "user = environ[\"USERNAME\"]\n",
    "datafolder= 'C:/Users/{:}/Tradeteq Dropbox/Tradeteq Team/Clients/#GoFactoring/data analysis/'.format(user)\n",
    "inputfilename = '09272018_instruments2.pkl'\n",
    "\n",
    "#feature selection\n",
    "feat_str = ['currency']\n",
    "\n",
    "feat_quant = ['has_purchase', 'dd_value_date', 'cd_lent_c', 'cd_repaid_c', 'cd_impaired1_c', 'cd_pastdue90_c', 'cd_trend_a',  'c_lent_c', \n",
    "             'c_repaid_c', 'c_impaired1_c', 'c_pastdue90_c', 'c_trend_a', 'cd_lent_c', 'd_repaid_c', 'd_impaired1_c', 'd_pastdue90_c',\n",
    "             'd_trend_a', 'd_we_payment_share']\n",
    "feat_exp = ['invoice_amount', 'purchase_amount']\n",
    "feat_date = ['invoice_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name_1</th>\n",
       "      <th>debtor_id</th>\n",
       "      <th>debtor_name_1</th>\n",
       "      <th>invoice_number</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>due_date</th>\n",
       "      <th>invoice_amount</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>purchase_amount_open</th>\n",
       "      <th>...</th>\n",
       "      <th>c_pastdue180_c</th>\n",
       "      <th>c_trend_a</th>\n",
       "      <th>c_we_payment_share</th>\n",
       "      <th>c_pd_mismatch_mean</th>\n",
       "      <th>c_pd_mismatch_std</th>\n",
       "      <th>c_repaid_r</th>\n",
       "      <th>c_impaired1_r</th>\n",
       "      <th>c_impaired2_r</th>\n",
       "      <th>c_pastdue90_r</th>\n",
       "      <th>c_pastdue180_r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2744:79/231</th>\n",
       "      <td>2004008</td>\n",
       "      <td>jobs united GmbH</td>\n",
       "      <td>79</td>\n",
       "      <td>Quadroni Linard</td>\n",
       "      <td>2744</td>\n",
       "      <td>2013-07-23</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>913.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861:79/232</th>\n",
       "      <td>2004008</td>\n",
       "      <td>jobs united GmbH</td>\n",
       "      <td>79</td>\n",
       "      <td>Quadroni Linard</td>\n",
       "      <td>2861</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>2013-08-09</td>\n",
       "      <td>2233.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932:79/233</th>\n",
       "      <td>2004008</td>\n",
       "      <td>jobs united GmbH</td>\n",
       "      <td>79</td>\n",
       "      <td>Quadroni Linard</td>\n",
       "      <td>2932</td>\n",
       "      <td>2013-08-06</td>\n",
       "      <td>2013-08-16</td>\n",
       "      <td>1370.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.185198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472:489/688</th>\n",
       "      <td>2004009</td>\n",
       "      <td>PM Personal GmbH</td>\n",
       "      <td>489</td>\n",
       "      <td>Style Interiors</td>\n",
       "      <td>1472</td>\n",
       "      <td>2013-08-13</td>\n",
       "      <td>2013-08-23</td>\n",
       "      <td>9195.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042:512/645</th>\n",
       "      <td>2004009</td>\n",
       "      <td>PM Personal GmbH</td>\n",
       "      <td>512</td>\n",
       "      <td>Elektropartner AG</td>\n",
       "      <td>2042</td>\n",
       "      <td>2013-08-13</td>\n",
       "      <td>2013-08-23</td>\n",
       "      <td>4594.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             customer_id   customer_name_1 debtor_id      debtor_name_1  \\\n",
       "uid                                                                       \n",
       "2744:79/231      2004008  jobs united GmbH        79    Quadroni Linard   \n",
       "2861:79/232      2004008  jobs united GmbH        79    Quadroni Linard   \n",
       "2932:79/233      2004008  jobs united GmbH        79    Quadroni Linard   \n",
       "1472:489/688     2004009  PM Personal GmbH       489    Style Interiors   \n",
       "2042:512/645     2004009  PM Personal GmbH       512  Elektropartner AG   \n",
       "\n",
       "             invoice_number invoice_date   due_date  invoice_amount  \\\n",
       "uid                                                                   \n",
       "2744:79/231            2744   2013-07-23 2013-08-02          913.70   \n",
       "2861:79/232            2861   2013-07-30 2013-08-09         2233.45   \n",
       "2932:79/233            2932   2013-08-06 2013-08-16         1370.50   \n",
       "1472:489/688           1472   2013-08-13 2013-08-23         9195.10   \n",
       "2042:512/645           2042   2013-08-13 2013-08-23         4594.60   \n",
       "\n",
       "              purchase_amount  purchase_amount_open  ... c_pastdue180_c  \\\n",
       "uid                                                  ...                  \n",
       "2744:79/231               0.0                   0.0  ...            0.0   \n",
       "2861:79/232               0.0                   0.0  ...            0.0   \n",
       "2932:79/233               0.0                   0.0  ...            0.0   \n",
       "1472:489/688              0.0                   0.0  ...            0.0   \n",
       "2042:512/645              0.0                   0.0  ...            0.0   \n",
       "\n",
       "              c_trend_a c_we_payment_share c_pd_mismatch_mean  \\\n",
       "uid                                                             \n",
       "2744:79/231    0.000000                NaN                NaN   \n",
       "2861:79/232    0.000000                NaN                NaN   \n",
       "2932:79/233    7.185198                NaN                NaN   \n",
       "1472:489/688   0.000000                NaN                NaN   \n",
       "2042:512/645   0.000000                NaN                NaN   \n",
       "\n",
       "             c_pd_mismatch_std  c_repaid_r c_impaired1_r c_impaired2_r  \\\n",
       "uid                                                                      \n",
       "2744:79/231                NaN         NaN           NaN           NaN   \n",
       "2861:79/232                NaN         0.0           0.0           0.0   \n",
       "2932:79/233                NaN         0.0           0.0           0.0   \n",
       "1472:489/688               NaN         NaN           NaN           NaN   \n",
       "2042:512/645               NaN         0.0           0.0           0.0   \n",
       "\n",
       "             c_pastdue90_r c_pastdue180_r  \n",
       "uid                                        \n",
       "2744:79/231            NaN            NaN  \n",
       "2861:79/232            0.0            0.0  \n",
       "2932:79/233            0.0            0.0  \n",
       "1472:489/688           NaN            NaN  \n",
       "2042:512/645           0.0            0.0  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri = pd.read_pickle(datafolder+inputfilename)\n",
    "ri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_str = [[i] for i in feat_str]\n",
    "feat_quant = [[j] for j in feat_quant]\n",
    "feat_exp = [[k] for k in feat_exp]\n",
    "feat_date = [[l] for l in feat_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "#convert datetimes to float\n",
    "class Date2Num(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        #print(X[0])\n",
    "        nanidx = pd.isnull(X)\n",
    "        X1 = np.zeros(X.shape)*np.nan\n",
    "        X1[~nanidx] = [float(pd.Timestamp(x).toordinal()) for x in X[~nanidx]]\n",
    "        return X1\n",
    "\n",
    "#nan replacer\n",
    "class ReplaceImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replacewith=999):\n",
    "        self.replacewith = replacewith\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X1 = X.copy()\n",
    "        X1[np.isnan(X1)] = self.replacewith\n",
    "        return X1\n",
    "\n",
    "#a log scaler to apply to some quant features\n",
    "class LogScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ZeroNegReplace=1e-5):\n",
    "        self.ZeroNegReplace = ZeroNegReplace\n",
    "        return\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X1=np.float32(X.copy())\n",
    "        nanidx = np.isnan(X1)\n",
    "        X2 = X1[~nanidx]\n",
    "        X2[X2 < self.ZeroNegReplace] = self.ZeroNegReplace\n",
    "        #badidx = X2 < 1e-10\n",
    "        #print('LogScaler: {:} nans, {:} bads'.format(sum(nanidx), sum(badidx)))\n",
    "        #if (sum(badidx)>0):\n",
    "        #    print(\"many bad indices!\")\n",
    "        #    print(X2[badidx])\n",
    "\n",
    "        X1[~nanidx] = np.log(X2)\n",
    "        return X1\n",
    "\n",
    "#cap the outliers greater than M std\n",
    "class CapOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, Maxstd=3.):\n",
    "        self.Maxstd = Maxstd\n",
    "    def fit(self, X, y=None):\n",
    "        self.mean = np.nanmean(X)\n",
    "        self.std = np.nanstd(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X1 = np.float32(X.copy())\n",
    "        nanidx = np.isnan(X1)\n",
    "        X2 = X1[~nanidx]\n",
    "        #print(\"CapOutliers: {:} nans, {:} mean, {:} std\".format(sum(nanidx), self.mean, self.std))\n",
    "        bigvals =  (np.abs(X2 - self.mean) > self.Maxstd * self.std)\n",
    "        X2[bigvals] = self.mean + self.Maxstd * self.std * np.sign(X2[bigvals] - self.mean)\n",
    "        X1[~nanidx] = X2\n",
    "        return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelines\n",
    "trans_date = gen_features(columns = feat_date,\n",
    "                          classes = [{'class': Date2Num},\n",
    "                                     {'class': CapOutliers, 'Maxstd': 4},\n",
    "                                     {'class': Imputer, 'strategy': \"mean\"},\n",
    "                                     {'class': StandardScaler}])\n",
    "\n",
    "trans_quant = gen_features(columns =  feat_quant, \n",
    "                               classes = [{'class': Imputer, 'strategy': \"mean\"},\n",
    "                                          {'class': CapOutliers, 'Maxstd': 4},\n",
    "                                          {'class': StandardScaler}])\n",
    "\n",
    "trans_exp = gen_features(columns = feat_exp, \n",
    "                               classes = [{'class': LogScaler, 'ZeroNegReplace': 1e-3},\n",
    "                                          {'class': CapOutliers, 'Maxstd': 4},\n",
    "                                          {'class': Imputer, 'strategy': \"mean\"}, \n",
    "                                          {'class': StandardScaler}])\n",
    "\n",
    "trans_str = gen_features(columns = feat_str, \n",
    "                             classes = [LabelBinarizer])\n",
    "\n",
    "preproc_pipeline = DataFrameMapper(trans_quant + trans_exp + trans_str + trans_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrameMapper(default=False, df_out=False,\n",
       "        features=[(['has_purchase'], [Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0), CapOutliers(Maxstd=4), StandardScaler(copy=True, with_mean=True, with_std=True)]), (['dd_value_date'], [Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0), CapOutlier...lues='NaN', strategy='mean', verbose=0), StandardScaler(copy=True, with_mean=True, with_std=True)])],\n",
       "        input_df=False, sparse=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2201 instruments that are not due yet, dropping...\n",
      "57619 instruments remaining\n"
     ]
    }
   ],
   "source": [
    "#drop all instruments that are not due yet\n",
    "print(\"{:} instruments that are not due yet, dropping...\".format(sum(~ri.is_due)))\n",
    "ri=ri.loc[ri.is_due, :]\n",
    "print(\"{:} instruments remaining\".format(ri.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 46095 for train and 11522 for test sets by shuffling...\n"
     ]
    }
   ],
   "source": [
    "# split training and test sets ('shuffle' and 'time' mode)\n",
    "\n",
    "trainsize = int(ri.shape[0]*.80)\n",
    "testsize = int(ri.shape[0]*.20)-1\n",
    "\n",
    "#for time splitting\n",
    "TestDate = pd.to_datetime('2018-09-01', yearfirst=True)\n",
    "control_feature = 'invoice_date'\n",
    "\n",
    "#for shuffle splitting \n",
    "testset_control_feature = 'invoice_date'\n",
    "\n",
    "split_mode = 'shuffle' #other option is 'time'\n",
    "\n",
    "if split_mode == 'shuffle':\n",
    "    print(\"Sampling {:} for train and {:} for test sets by shuffling...\".format(trainsize, testsize))\n",
    "\n",
    "    ri[\"invoice_date_year\"] = ri[testset_control_feature].apply(lambda x: x.year)\n",
    "\n",
    "        \n",
    "    split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                    train_size = trainsize, \n",
    "                                    test_size = testsize, \n",
    "                                    random_state=42)\n",
    "\n",
    "    ri = ri.reset_index(drop=True)\n",
    "    \n",
    "    #constructing oversampled class y=1 train and test sets:\n",
    "    for train_index, test_index in split.split(ri, ri.invoice_date_year):\n",
    "        train_all = ri.loc[train_index]\n",
    "        test_all = ri.loc[test_index]\n",
    "    \n",
    "elif split_mode == 'time':\n",
    "    print(\"Splitting train and test sets by time, test cutoff: {:}...\".format(TestDate))\n",
    "    test_all  = ri.loc[ri[control_feature] >= TestDate]\n",
    "    train_all = ri.loc[ri[control_feature] <  TestDate]\n",
    "    print(\"  {:}({:.1f}%) train, {:}({:.1f}%) test\".format(train_all.shape[0], 100*train_all.shape[0]/ri.shape[0],\n",
    "                                                            test_all.shape[0],   100*test_all.shape[0]/ri.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING THE TARGET FEATURE (it could be 'has_impairment1', 'is_pastdue90', 'is_pastdue180')\n",
    "targetfeature = 'has_impairment1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the pipeline, target feature is has_impairment1...\n",
      "Train y: 46095 total, 968 (2.10%) > 0\n",
      "pipeline fit_transform for train set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the pipeline, target feature is {:}...\".format(targetfeature))\n",
    "\n",
    "#prepare and save train sets\n",
    "#separate features and labels\n",
    "y_train = train_all[targetfeature].copy().values\n",
    "print(\"Train y: {:} total, {:} ({:.2f}%) > 0\".format(y_train.shape[0], sum(y_train>0), sum(y_train>0)/y_train.shape[0]*100))\n",
    "#apply the pipeline to the training set\n",
    "print(\"pipeline fit_transform for train set...\")\n",
    "X_train = preproc_pipeline.fit_transform(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test y: 11522 total, 227 (1.97%) > 0\n",
      "pipeline transform only for test set...\n"
     ]
    }
   ],
   "source": [
    "#prepare and save test sets\n",
    "#separate features and labels\n",
    "y_test = test_all[targetfeature].copy().values\n",
    "print(\"Test y: {:} total, {:} ({:.2f}%) > 0\".format(y_test.shape[0], sum(y_test>0), sum(y_test>0)/y_test.shape[0]*100))\n",
    "#apply the pipeline to the training set\n",
    "print(\"pipeline transform only for test set...\")\n",
    "X_test = preproc_pipeline.transform(test_all) #will be a problem if new categories are encountered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the category labels together for charting\n",
    "feature_labels = preproc_pipeline.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix = '_imp1'\n",
    "\n",
    "outputfolder = ''\n",
    "\n",
    "#saving training and test sets\n",
    "print(\"Saving with file name postfix {:}...\".format(postfix))\n",
    "pickle.dump([X_train, y_train, feature_labels], open(outputfolder+\"traindata\" + postfix, \"wb\"), protocol=4)\n",
    "pickle.dump([X_test, y_test, feature_labels], open(outputfolder+\"testdata\" + postfix, \"wb\"), protocol=4)\n",
    "pickle.dump(preproc_pipeline, open(outputfolder+\"preproc_pipeline\" + postfix, \"wb\"))\n",
    "pickle.dump(feature_labels, open(outputfolder+\"feature_labels\" + postfix, \"wb\"))\n",
    "print(\"...done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
