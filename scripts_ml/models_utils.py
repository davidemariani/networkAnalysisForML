#==================================================================================#
# Author       : Davide Mariani                                                    #  
# University   : Birkbeck College, University of London                            # 
# Programme    : Msc Data Science                                                  #
# Script Name  : models_utils.py                                                   #
# Description  : utils for ML modelling, implementation and prototipation          #
# Version      : 0.9                                                               #
#==================================================================================#
# This file contains functions do implement and prototype ML models using          #
# scikit learn                                                                     #
#==================================================================================#

#importing main modules
import pandas as pd
import numpy as np
import pickle
import datetime 
import os

from sklearn.model_selection import cross_val_predict, cross_validate
from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, confusion_matrix
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import clone

from scripts_mlflow.mlflow_utils import *


#-----------------------------------------
# EXPERIMENTS RUN
#-----------------------------------------

def models_loop(models, datafolder, prefixes, postfixes, trainfile='_traindata', testfile='_testdata', scoring = {'AUC':'roc_auc'},
                CrossValFolds=5, timeSeqValid = False, train_window = 6000, test_window = 1200, 
                save_model=False, models_path='../data/models/', mlf_tracking=False, experiment_name='experiment',
                save_results_for_viz=False, viz_output_path='../data/viz_data/'):
    """
    This function's main purpose is running a full experiment from validation to testing for evaluating performances.
    It requires as inputs:
    - models: a list of models
    - datafolder: the path of the folder containing train and test files generated by the preprocessing pipeline
    - prefixes: the prefix for each of them (list)
    - postfixes: the postfixes for each of them (list)
    - trinfile and testfile: traindata and testdata name are default
    - scoring: dictionary with the scoring methods for cross validation
    - CrossValFolds: number of folds for cross validation phase
    - timeSeqValid: if True, time sequence validation will be performed (this has to be done after a TIME SPLIT PREPROCESSING otherwise it doesn't make sense!
      Please be sure that the experiment name contain 'time'!!!)
    - train_window: size of the training window  (this works for timeSeqValid=True)
    - test_window: size of the testing window (this works for timeSeqValid=True)
    - save_model: option to save the model, give it an output path and model name
    - option to save AUC visualization data as pickles in specific folder
    - option of tracking the experiments in mlflow

    It returns a dictionary containing the results from validation and testing phase, useful to be plugged in the plot_rocs function
    to visualize AUCs.

    It also allow results and hyperparameters tracking in MLflow setting mlf_tracking to True. 
    """

    #check that the lists have consistent length
    if len(prefixes) == len(postfixes):
        pass
    else:
        print("Inputs length is inconsistent! Please check that number of prefixes and postfixes is the same.")
        return
    
    for p in range(len(prefixes)): #loop by credit event (imp, p90, p180)
        
        prefix = prefixes[p]
        postfix = postfixes[p]

        for loop in range(len(models)):

            print("----Loop {} of {} for credit event {}----".format(loop+1, len(models), prefixes[p]))

            #results storage dictionary
            results = {'experiment':experiment_name,
                       'prefix':prefix,
                       'postfix':postfix}

            #selecting model and transformed train and test sets
            model = models[loop]

            modeltype = str(model).split('(')[0]
            results = {'model':modeltype}

            print("Training, validation and testing of experiment with prefix {} and postfix {} using {}".format(prefix, postfix, modeltype))

            #loading preprocessed data
            trainfilepath = datafolder + prefix + trainfile + postfix+'.pkl'
            testfilepath = datafolder + prefix + testfile + postfix+'.pkl'

            print("-Loading preprocessed data...")
            print("training files: {}".format(trainfilepath))
            print("testing files: {}".format(testfilepath))
            [X_train, y_train, feature_labels] = pd.read_pickle(trainfilepath) 
            [X_test, y_test, feature_labels] = pd.read_pickle(testfilepath) 

            #validation performance
            print('- Training/Validation...')
            if timeSeqValid:
                model_kfold = model_diag_seq(model, X_train, y_train, 
                                             train_window=train_window, test_window=test_window, scoring=scoring)
            else:
                model_kfold = model_diag(model, X_train, y_train, CrossValFolds=CrossValFolds, scoring=scoring)
            results['validation'] = model_kfold

            #fitting model
            print('- Training for test...')
            model.fit(X_train, y_train)

            #testing on out of sample observations
            print('- Testing...')
            model_oos = model_oostest(model, X_test, y_test)
            results['testing'] = model_oos

            #retrieving model's feature importances
            f_imp = get_features_importances(model, feature_labels)
            results['feature_importances'] = f_imp

            #saving the model
            if save_model:
                output_path = models_path+experiment_name+'/'
                print('- Saving the model to {}...'.format(output_path))
                filename, filepath = save_sk_model(model, output_path, modeltype, prefix)
            else: #for consistency with mlf_tracking
                filename = None
                filepath = None

            #saving results dictionary for viz
            if save_results_for_viz:
                if not save_model: #generating filename in the case save_model=False (to have names consistency, in case it is True, viz name and model name will be the same)
                    filename = encode_name(modeltype, prefix)
                dict_name = save_dictionary(results, viz_output_path+experiment_name+'/', filename.split('.')[0],'_viz')
            else:
                dict_name = None #forcing dict_name to have a value for mlflow tracking consistency (case where save_results_for_viz=False and mlf_tracking=True)

            if mlf_tracking: #mlflow tracking

                mlf_sk_tracking(experiment_name, prefix, postfix, modeltype, trainfile, testfile, datafolder,
                    X_train.shape[0], X_test.shape[0], model, model_kfold, model_oos, timeSeqValid, f_imp, CrossValFolds, 
                    save_model=save_model, filename=filename, filepath=filepath, save_results_for_viz=save_results_for_viz, dict_name=dict_name)

            print()
    print("Experiment done!")
    return results


def models_loop_time_leak(models, datafolder, prefixes, postfixes, val_timeseq_encoding, val_postfixes, indexfile,
                          trainfile='_traindata', testfile='_testdata', scoring = {'AUC':'roc_auc'},
                save_model=False, models_path='../data/models/', mlf_tracking=False, experiment_name='experiment',
                save_results_for_viz=False, viz_output_path='../data/viz_data/'):
    """
    This function's main purpose is running a full experiment from validation to testing in a similar way to models_loop.
    The difference is that in this case time leak is prevented and the data used are assumed to come from a specific preprocessing
    pipeline giving separate set of files for validation and testing.
    It requires as inputs:
    - models: a list of models
    - datafolder: the path of the folder containing train and test files generated by the preprocessing pipeline
    - prefixes: the prefix for each of them (list)
    - postfixes: the postfixes for each of them (list)
    - val_timeseq_encoding: the portion of the validation file's name indicating the windows sizes used (it will be used to retrieve the validation files)
    - val_postfixes: the postfixes for each of the validation files
    - indexfile: the file storing the indexes for validation splits
    - trinfile and testfile: traindata and testdata name are default
    - scoring: dictionary with the scoring methods for cross validation
    - save_model: option to save the model, give it an output path and model name
    - save_results_for_viz: option to save AUC visualization data as pickles in specific folder viz_output_path
    - mlf_tracking: option of tracking the experiments in mlflow

    It returns a dictionary containing the results from validation and testing phase, useful to be plugged in the plot_rocs function
    to visualize AUCs.

    It also allow results and hyperparameters tracking in MLflow setting mlf_tracking to True. 
    """

    #check that the lists have consistent length
    if len(prefixes) == len(postfixes):
        pass
    else:
        print("Inputs length is inconsistent! Please check that number of prefixes and postfixes is the same.")
        return
    
    for p in range(len(prefixes)): #loop by credit event (imp, p90, p180)
        
        prefix = prefixes[p]
        postfix = postfixes[p]
        val_postfix = val_postfixes[p]

        for loop in range(len(models)):

            print("----Loop {} of {} for credit event {}----".format(loop+1, len(models), prefixes[p]))

            #results storage dictionary
            results = {'experiment':experiment_name,
                       'prefix':prefix,
                       'postfix':postfix}

            #selecting model and transformed train and test sets
            model = clone(models[loop])

            modeltype = str(model).split('(')[0]
            results = {'model':modeltype}

            print("Training, validation and testing of experiment with prefix {} and postfix {} using {}".format(prefix, postfix, modeltype))

            #loading preprocessed data for validation
            val_trainfilepath = datafolder + prefix + val_timeseq_encoding + trainfile + val_postfix+'.pkl'
            val_testfilepath = datafolder + prefix + val_timeseq_encoding + testfile + val_postfix+'.pkl'
            val_indexespath = datafolder + prefix + val_timeseq_encoding + indexfile + val_postfix+'.pkl'

            print("--------------Loading VALIDATION preprocessed data...")
            print("training files: {}".format(val_trainfilepath))
            print("testing files: {}".format(val_testfilepath))
            [val_X_train, val_y_train, feature_labels] = pd.read_pickle(val_trainfilepath) 
            [val_X_test, val_y_test, feature_labels] = pd.read_pickle(val_testfilepath) 
            indexes = pd.read_pickle(val_indexespath)

            #validation performance
            print('- Training/Validation...')
            model_kfold = model_diag_seq(model, val_X_train, val_y_train, 
                                             specify_idxs=True, idx_tuples=indexes, X_test=val_X_test, y_test=val_y_test, scoring=scoring)

            results['validation'] = model_kfold

            #fitting model with separate train-test preprocessed data
            print('- Training for test...')
            #loading preprocessed data for validation
            trainfilepath = datafolder + prefix + trainfile + postfix+'.pkl'
            testfilepath = datafolder + prefix + testfile + postfix+'.pkl'

            print("---------------Loading TEST preprocessed data...")
            print("training files: {}".format(trainfilepath))
            print("testing files: {}".format(testfilepath))
            [X_train, y_train, feature_labels] = pd.read_pickle(trainfilepath) 
            [X_test, y_test, feature_labels] = pd.read_pickle(testfilepath) 

            #training the model
            print("- Training the model..")
            model = clone(models[loop])
            model.fit(X_train, y_train)

            #testing on out of sample observations
            print('- Testing...')
            model_oos = model_oostest(model, X_test, y_test)
            results['testing'] = model_oos

            #retrieving model's feature importances
            f_imp = get_features_importances(model, feature_labels)
            results['feature_importances'] = f_imp

            #saving the model
            if save_model:
                output_path = models_path+experiment_name+'/'
                print('- Saving the model to {}...'.format(output_path))
                filename, filepath = save_sk_model(model, output_path, modeltype, prefix)
            else: #for consistency with mlf_tracking
                filename = None
                filepath = None

            #saving results dictionary for viz
            if save_results_for_viz:
                if not save_model: #generating filename in the case save_model=False (to have names consistency, in case it is True, viz name and model name will be the same)
                    filename = encode_name(modeltype, prefix)
                dict_name = save_dictionary(results, viz_output_path+experiment_name+'/', filename.split('.')[0],'_viz')
            else:
                dict_name = None #forcing dict_name to have a value for mlflow tracking consistency (case where save_results_for_viz=False and mlf_tracking=True)

            if mlf_tracking: #mlflow tracking

                mlf_sk_tracking(experiment_name, prefix, postfix, modeltype, trainfile, testfile, datafolder,
                    X_train.shape[0], X_test.shape[0], model, model_kfold, model_oos, True, f_imp, CrossValFolds=len(indexes), 
                    save_model=save_model, filename=filename, filepath=filepath, save_results_for_viz=save_results_for_viz, dict_name=dict_name)

            print()
    print("Experiment done!")
    return results
        

#-----------------------------------------
# VALIDATION METHODS UTILS
#-----------------------------------------

def rolling_window(T, ntrain, ntest, gen_for_grid_search=False,
                   fixed_test_folds_idxs=False):
    """
    This function executes a generator for performing 'rolling window validation', in which a sliding portion of the training
    set is used  instead of classical k-fold validation.
    It asks for the size of the training set, the (indicative) number of training observation for each time fold and the number of test observations
    for each time fold.
    The generator will yield fold-count number, train indexes and test indexes.
    Setting gen_for_grid_search to True will make the generator yielding just train indexes and test indexes; this is useful when using gridsearch
    for hyperparameters tuning in scikit learn, creating customized validation options with the attribute 'cv'.
    Setting fixed_test_folds_size = True and specifying a test_fold_size, the generator will yield progressive
    """

    Nsteps = (T - ntrain) // ntest #rounded down number of folds

    starti = 0 #running first index of training set

    for count in range(int(Nsteps)): #for each fold
        traini = np.array(range(int(starti), int(T - (Nsteps-count)*ntest))) #may be longer than ntrain for count==0
        testi = np.array(range(int(T - (Nsteps-count)*ntest), int(T - (Nsteps-count)*ntest + ntest)))
        starti = int(T - (Nsteps-count-1)*ntest - ntrain)

        print("Preparing fold {} with {} train observations and {} test observations, starti={}...".format(count, len(traini), len(testi), starti))

        if gen_for_grid_search: #option for 'cv' in scikit-learn grid_search or random_search
            if fixed_test_folds_idxs: #if there's always the same size for test fold split, generate progressive folds
                if count==0:
                    testi_gs = np.array(range(0, ntest))
                else:
                    testi_gs+=ntest

                yield traini, testi_gs

            else:
                yield traini, testi
        else:
            yield count, traini, testi, Nsteps


def rolling_window_idxs(indexes_tuples):
    """
    This function is a generator producing a similar output to rolling_window but the indexes are given as inputs and then yielded 
    to be used in cv for GridSearch
    """
    for tup in indexes_tuples:
        yield tup[0], tup[1]


#-----------------------------------------
# VALIDATION METHODS
#-----------------------------------------

def model_diag(model, X_train, y_train, CrossValFolds=5, scoring = {'AUC':'roc_auc'}):
    """
    This function returns as output false positive rates, true positive rates and auc score for stratified kfold and each
    cross validation fold in the form of a dictionary.
    It needs model, training x and training y as inputs.
    """

    validation = cross_validate(model, X_train, y_train, cv=CrossValFolds, scoring=scoring) #cross_validate is used to evaluate each fold separately

    if hasattr(model, "decision_function"): #cross_val_predict is used to make predictions on each data point using stratified folds (evaluation of the whole set)
        y_scores = cross_val_predict(model, X_train, y_train, cv=CrossValFolds, method="decision_function")
    else:
        y_proba = cross_val_predict(model, X_train, y_train, cv=CrossValFolds, method="predict_proba")
        y_scores = y_proba[:,1]

    fpr, tpr, thresholds = roc_curve(y_train, y_scores) #false positive rates, true positive rates and thresholds
    auc = roc_auc_score(y_train, y_scores)
    
    print("AUC {:.3f}".format(auc))

    results = {'fpr':fpr, 'tpr':tpr, 'auc':auc}

    for score in list(scoring.keys()):
        for fold in range(1, len(validation['test_'+score])+1): #saving auc score at each fold of cross validation
            results[score+'_fold_'+str(fold)] = validation['test_'+score][fold-1]

    return results



def model_diag_seq(model, X_train, y_train, specify_idxs=False, train_window=6000, test_window=1200,
                   idx_tuples=[] , X_test=[], y_test=[], scoring = {'AUC':'roc_auc'}):
    """
    This function returns as output false positive rates, true positive rates and auc score for time sequence fold validation
    and each time split validation fold in the form of a dictionary.
    It needs model, training x, training y and windows dimensions as inputs if used with a generator (specify_idxs=False, which is the default option).
    It can be used specifying  indexes for fold splitting setting specify_idxs to True, passing a list of tuples ([train_indexes], [test_indexes])
    for each fold and providing separate X_test and y_test.
    The latter option is implemented to be used in time sequential validation experiments preventing the time leak problem, so it needs to be used
    with preprocessed dataset created with the specific time sequence pipeline. 
    The first option is used for time sequential validation without preventing time leak problem (it just sequentially splits the dataset in train and test folds).
    """
    
    results = {} #results storage dictionary

    preds = [] #lists for combining all the folds and evaluating overall performance
    tests = []

    #split set in timewise folds using a generator
    if not specify_idxs: #in this case a rolling window generator is used (default option)

        T = X_train.shape[0]
        if train_window<1: #given as share of T
            train_window = np.floor(train_window*T)
        if test_window<1:
            test_window = np.floor(test_window*T)

        fold_generator = rolling_window(T, train_window, test_window)

        for count, train_idx, test_idx, Nsteps in fold_generator: #looping through validation folds specified by rolling window generator
            
            #validate and test
            auc_fold, y_fold_test, y_scores = validate_and_test(model, X_train, y_train, train_idx, test_idx, count)

            #storing fold results
            for score in list(scoring.keys()):
                results[score+'_fold_'+str(count+1)] = auc_fold
            
            preds+=list(y_scores)
            tests+=list(y_fold_test)
    else:
        if len(idx_tuples)==0:
            print("No indexes for train and test portions provided! Please include a list of tuples for specifying them!")
            return
        else:
            count=0

            if len(X_test)==0 or len(y_test)==0:
                print("WARNING! You are using idx_tuples=True without specifying X_test and y_test! Please provide them to perform validation on index specified sets!")
                return

            for idxs in idx_tuples: #looping through validation folds specified by indexes
                train_idx = idxs[0]
                if count==0:
                    test_idx = np.array(range(0, len(idxs[1])))
                    count+=1
                else:
                    test_idx+=len(idxs[1])
                    count+=1

                #validate and test
                auc_fold, y_fold_test, y_scores = validate_and_test(model, X_train, y_train, train_idx, test_idx, count,
                                                                    from_separate_sets=True, X_test=X_test, y_test=y_test)

                #storing fold results
                for score in list(scoring.keys()):
                    results['AUC_fold_'+str(count)] = auc_fold

                preds+=list(y_scores)
                tests+=list(y_fold_test)

    #storing results
    fpr, tpr, thresholds = roc_curve(tests, preds) #false positive rates, true positive rates and thresholds
    auc = roc_auc_score(tests, preds)
    print("Validation AUC {:.3f}".format(auc))

    results['fpr'] = fpr
    results['tpr'] = tpr
    results['auc'] = auc

    return results


def validate_and_test(model, X_train, y_train, train_idx, test_idx, count, from_separate_sets=False, X_test=None, y_test=None):
    """
    Auxiliar function for model_diag_seq where custom fold validation is implemented.
    It takes data for a single fold returning corresponding auc
    It  requires as inputs:
    - the classifier to validate
    - X observations
    - y labels
    - indexes for X set
    - indexes for y set
    - count indicates the number of the iteration during the loop (and the number of the fold as well
    - from_separate_sets set to True indicate that X_test and y_test need to be provided separately, otherwise it assumes that
      a generator for fold splitting has been used
    - X_test and y_test if from_separate_sets=True)

    It will return auc score, test points of the fold, predictions for each test point of the fold.
    """

    train_idx = train_idx.astype(int)
    test_idx = test_idx.astype(int)
    X_fold_train = X_train[train_idx]
    y_fold_train = y_train[train_idx]

    if from_separate_sets:
        if len(X_test)==0 or len(y_test)==0:
            print("WARNING! You are using idx_tuples=True without specifying X_test and y_test! Please provide them to perform validation on index specified sets!")
            return
        X_fold_test = X_test[test_idx]
        y_fold_test = y_test[test_idx]

    else:
        X_fold_test = X_train[test_idx]
        y_fold_test = y_train[test_idx]

    print("Fold {:}: train  on {:} from index {:} to {:}, test on {:} from {:} to {:}".format(count,
                    len(X_fold_train), train_idx[0], train_idx[-1], len(X_fold_test), test_idx[0], test_idx[-1]))

    model.fit(X_fold_train, y_fold_train)
    
    if hasattr(model, "decision_function"): 
        y_scores = model.decision_function(X_fold_test) 
    else:
        y_proba = model.predict_proba(X_fold_test)
        y_scores = y_proba[:,1]

    auc = roc_auc_score(y_fold_test, y_scores)

    print("Fold {} AUC: {}".format(count, auc))

    return auc, y_fold_test, y_scores



#-----------------------------------------
# TESTING METHODS
#-----------------------------------------

def model_oostest(model, X_test, y_test):
    """
    This function tests the model performance on out of sample data
    """

    #cm
    y_score = model.predict(X_test)
    cm = confusion_matrix(y_test, y_score)
    rcm = cm_ratio(cm)

    #metrics
    y_scores = model.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, y_scores)
    auc = roc_auc_score(y_test, y_scores)

    print("AUC {:.3f}".format(auc))

    return {'fpr':fpr, 'tpr':tpr, 'auc':auc, 'rcm':rcm, 'cm':cm, 'predictions':y_scores}


#-----------------------------------------
# MODEL DIAGNOSTICS UTILS
#-----------------------------------------


def cm_ratio(cm):
    """
    This function takes as an input a scikit learn confusion matrix and returns it with ratio values.
    """

    rcm = np.empty([2,2])
    rcm[0, :] = cm[0, :] / float(sum(cm[0, :]))
    rcm[1, :] = cm[1, :] / float(sum(cm[0, :]))
        
    print("Confusion matrix: \n" + np.array_str(rcm, precision=5, suppress_small=True))

    return rcm


def get_features_importances(trained_model, feature_labels):
    """
    This functions, given a trained model and the labels of the features used to train it,
    will return a dictionary with the weights (for linear models) or the importance (for tree models)
    for every feature
    """

    feat_imp = {}

    if hasattr(trained_model, "feature_importances_"):
        importances = trained_model.feature_importances_
    
    elif hasattr(trained_model, "coef_"):
        importances = trained_model.coef_[0]

    else:
        print("None of the weights/importance module found...")
        return feat_imp

    feat_imp = dict(zip(feature_labels, importances))
    return feat_imp

#-----------------------------------------
# DATA AND MODEL SAVING UTILS
#-----------------------------------------


def encode_name(model_name, prefix):
    """
    This function create a unique name for each model based on datetime.
    It will output a name in the format: prefix_modelname_yearmonthday_hourminsec
    """

    year = str(datetime.datetime.now().year)[2:]
    month = str(datetime.datetime.now().month)
    if len(month)==1:
        month = '0'+month
    day = str(datetime.datetime.now().day)
    if len(day)==1:
        day = '0'+day

    hour = str(datetime.datetime.now().hour)
    min = str(datetime.datetime.now().minute)
    sec = str(datetime.datetime.now().second)

    postfix = '_'+year+month+day+'_'+hour+min+sec
    filename = prefix +'_'+ model_name + postfix+'.pkl'

    return filename


def save_sk_model(model, datafolder, model_name, prefix):
    """
    This function saves a scikit learn model in pickle format
    """

    filename = encode_name(model_name, prefix) #creating file name

    filepath = datafolder+filename

    # Create target folder if it doesn't exist
    if not os.path.exists(datafolder):
        os.mkdir(datafolder)

    print("- Saving model to {}".format(filepath))
    with open(filepath, "wb") as pickle_file:
            pickle.dump(model, pickle_file)

    return filename, filepath


def save_dictionary(dict, datafolder, dict_name, postfix):
    """
    This function saves a dictionary to a pickle file
    """
    filepath = datafolder+dict_name+postfix+'.pkl'

    # Create target folder if it doesn't exist
    if not os.path.exists(datafolder):
        os.mkdir(datafolder)

    print("- Saving dictionary to {}".format(filepath))
    with open(filepath, "wb") as pickle_file:
            pickle.dump(dict, pickle_file)
    return filepath
   


#-----------------------------------------
# MAIN
#-----------------------------------------

def main():
    print("models_utils.py executed/loaded..")

if __name__ == "__main__":
    main()