#==================================================================================#
# Author       : Davide Mariani                                                    #  
# University   : Birkbeck College, University of London                            # 
# Programme    : Msc Data Science                                                  #
# Script Name  : models_utils.py                                                   #
# Description  : utils for ML modelling, implementation and prototipation          #
# Version      : 0.9                                                               #
#==================================================================================#
# This file contains functions do implement and prototype ML models using          #
# scikit learn                                                                     #
#==================================================================================#

#importing main modules
import pandas as pd
import numpy as np
import pickle
import datetime 
import os

from sklearn.model_selection import cross_val_predict, cross_validate
from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, confusion_matrix
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier

import mlflow
import mlflow.sklearn


#-----------------------------------------
# EXPERIMENTS RUN
#-----------------------------------------

def models_loop(models, datafolder, prefixes, postfixes, trainfile='_traindata', testfile='_testdata', scoring = {'AUC':'roc_auc'},
                CrossValFolds=5, timeSeqValid = False, train_window = 6000, test_window = 1200, 
                save_model=False, models_path='../data/models/', mlf_tracking=False, experiment_name='experiment',
                save_results_for_viz=False, viz_output_path='../data/viz_data/'):
    """
    This function's main purpose is the comparison between validation and testing in order to tune the model during calibration.
    It performs training, validation and testing of one or more models on one or more credit events, requiring as inputs:
    - models: a list of models
    - datafolder: the path of the folder containing train and test files generated by the preprocessing pipeline
    - prefixes: the prefix for each of them (list)
    - postfixes: the postfixes for each of them (list)
    - trinfile and testfile: traindata and testdata name are default
    - scoring: dictionary with the scoring methods for cross validation
    - CrossValFolds: number of folds for cross validation phase
    - timeSeqValid: if True, time sequence validation will be performed (this has to be done after a TIME SPLIT PREPROCESSING otherwise it doesn't make sense!
      Please be sure that the experiment name contain 'time'!!!)
    - train_window: size of the training window  (this works for timeSeqValid=True)
    - test_window: size of the testing window (this works for timeSeqValid=True)
    - save_model: option to save the model, give it an output path and model name
    - option to save AUC visualization data as pickles in specific folder
    - option of tracking the experiments in mlflow

    It returns a dictionary containing the results from validation and testing phase, useful to be plugged in the plot_rocs function
    to visualize AUCs.

    It also allow results and hyperparameters tracking in MLflow setting mlf_tracking to True. 
    """

    #check that the lists have consistent length
    if len(prefixes) == len(postfixes):
        pass
    else:
        print("Inputs length is inconsistent! Please check that number of prefixes and postfixes is the same.")
        return
    
    for p in range(len(prefixes)): #loop by credit event (imp, p90, p180)
        
        prefix = prefixes[p]
        postfix = postfixes[p]

        for loop in range(len(models)):

            print("----Loop {} of {} for credit event {}----".format(loop+1, len(models), prefixes[p]))

            #results storage dictionary
            results = {'experiment':experiment_name,
                       'prefix':prefix,
                       'postfix':postfix}

            #selecting model and transformed train and test sets
            model = models[loop]

            modeltype = str(model).split('(')[0]
            results = {'model':modeltype}

            print("Training, validation and testing of experiment with prefix {} and postfix {} using {}".format(prefix, postfix, modeltype))

            #loading preprocessed data
            trainfilepath = datafolder + prefix + trainfile + postfix+'.pkl'
            testfilepath = datafolder + prefix + testfile + postfix+'.pkl'

            print("-Loading preprocessed data...")
            print("training files: {}".format(trainfilepath))
            print("testing files: {}".format(testfilepath))
            [X_train, y_train, feature_labels] = pd.read_pickle(trainfilepath) 
            [X_test, y_test, feature_labels] = pd.read_pickle(testfilepath) 

            #validation performance
            print('- Training/Validation...')
            if timeSeqValid:
                model_kfold = model_diag_seq(model, X_train, y_train, 
                                             train_window=train_window, test_window=test_window, scoring=scoring)
            else:
                model_kfold = model_diag(model, X_train, y_train, CrossValFolds=CrossValFolds, scoring=scoring)
            results['validation'] = model_kfold

            #fitting model
            print('- Training for test...')
            model.fit(X_train, y_train)

            #testing on out of sample observations
            print('- Testing...')
            model_oos = model_oostest(model, X_test, y_test)
            results['testing'] = model_oos

            #retrieving model's feature importances
            f_imp = get_features_importances(model, feature_labels)
            results['feature_importances'] = f_imp

            #saving the model
            if save_model:
                output_path = models_path+experiment_name+'/'
                print('- Saving the model to {}...'.format(output_path))
                filename, filepath = save_sk_model(model, output_path, modeltype, prefix)

            #saving results dictionary for viz
            if save_results_for_viz:
                if not save_model: #generating filename in the case save_model=False (to have names consistency, in case it is True, viz name and model name will be the same)
                    filename = encode_name(modeltype, prefix)
                dict_name = save_dictionary(results, viz_output_path+experiment_name+'/', filename.split('.')[0],'_viz')
            else:
                dict_name = None #forcing dict_name to have a value for mlflow tracking consistency (case where save_results_for_viz=False and mlf_tracking=True)

            if mlf_tracking: #mlflow tracking

                mlf_sk_tracking(experiment_name, prefix, postfix, modeltype, trainfile, testfile, datafolder,
                    X_train.shape[0], X_test.shape[0], model, model_kfold, model_oos, timeSeqValid, f_imp, CrossValFolds, 
                    save_model=False, filename=None, filepath=None, save_results_for_viz=save_results_for_viz, dict_name=dict_name)

            print()
    print("Experiment done!")
    return results
        

#-----------------------------------------
# VALIDATION METHODS UTILS
#-----------------------------------------

def rolling_window(T, ntrain, ntest, gen_for_grid_search=False):
    """
    This function executes a generator for performing 'rolling window validation', in which a sliding portion of the training
    set is used  instead of classical k-fold validation.
    It asks for the size of the training set, the (indicative) number of training observation for each time fold and the number of test observations
    for each time fold.
    The generator will yield fold-count number, train indexes and test indexes.
    Setting gen_for_grid_search to True will make the generator yielding just train indexes and test indexes; this is useful when using gridsearch
    for hyperparameters tuning in scikit learn, creating customized validation options with the attribute 'cv'.
    """

    Nsteps = (T - ntrain) // ntest #rounded down number of folds

    starti = 0 #running first index of training set

    for count in range(int(Nsteps)): #for each fold
        traini = np.array(range(int(starti), int(T - (Nsteps-count)*ntest))) #may be longer than ntrain for count==0
        testi = np.array(range(int(T - (Nsteps-count)*ntest), int(T - (Nsteps-count)*ntest + ntest)))
        starti = int(T - (Nsteps-count-1)*ntest - ntrain)

        print("Preparing fold {} with start at {}, {} train observations and {} test observations...".format(count, starti, len(traini), len(testi)))

        if gen_for_grid_search: #option for 'cv' in grid_search
            yield traini, testi
        else:
            yield count, traini, testi, Nsteps


#-----------------------------------------
# VALIDATION METHODS
#-----------------------------------------

def model_diag(model, X_train, y_train, CrossValFolds=5, scoring = {'AUC':'roc_auc'}):
    """
    This function returns as output false positive rates, true positive rates and auc score for stratified kfold and each
    cross validation fold in the form of a dictionary.
    It needs model, training x and training y as inputs.
    """

    validation = cross_validate(model, X_train, y_train, cv=CrossValFolds, scoring=scoring) #cross_validate is used to evaluate each fold separately

    if hasattr(model, "decision_function"): #cross_val_predict is used to make predictions on each data point using stratified folds (evaluation of the whole set)
        y_scores = cross_val_predict(model, X_train, y_train, cv=CrossValFolds, method="decision_function")
    else:
        y_proba = cross_val_predict(model, X_train, y_train, cv=CrossValFolds, method="predict_proba")
        y_scores = y_proba[:,1]

    fpr, tpr, thresholds = roc_curve(y_train, y_scores) #false positive rates, true positive rates and thresholds
    auc = roc_auc_score(y_train, y_scores)
    
    print("AUC {:.3f}".format(auc))

    results = {'fpr':fpr, 'tpr':tpr, 'auc':auc}

    for score in list(scoring.keys()):
        for fold in range(1, len(validation['test_'+score])+1): #saving auc score at each fold of cross validation
            results[score+'_fold_'+str(fold)] = validation['test_'+score][fold-1]

    return results



def model_diag_seq(model, X_train, y_train, train_window=6000, test_window=1200, scoring = {'AUC':'roc_auc'}):
    """
    This function returns as output false positive rates, true positive rates and auc score for time sequence fold validation
    and each time split validation fold in the form of a dictionary.
    It needs model, training x, training y and windows dimensions as inputs.
    """
    
    T = X_train.shape[0]
    if train_window<1: #given as share of T
        train_window = np.floor(train_window*T)
    if test_window<1:
        test_window = np.floor(test_window*T)

    fold_generator = rolling_window(T, train_window, test_window)

    results = {}

    preds = [] #lists for combining all the folds and evaluating overall performance
    tests = []

    #split set in timewise folds
    for count, train_idx, test_idx, Nsteps in fold_generator:
        train_idx = train_idx.astype(int)
        test_idx = test_idx.astype(int)
        X_fold_train = X_train[train_idx]
        y_fold_train = y_train[train_idx]
        X_fold_test = X_train[test_idx]
        y_fold_test = y_train[test_idx]

        print("Fold {:}: train  on {:} from index {:} to {:}, test on {:} from {:} to {:}".format(count,
                        len(X_fold_train), train_idx[0], train_idx[-1], len(X_fold_test), test_idx[0], test_idx[-1]))

        model.fit(X_fold_train, y_fold_train)
    
        if hasattr(model, "decision_function"): #cross_val_predict is used to make predictions on each data point using stratified folds (evaluation of the whole set)
            y_scores = model.decision_function(X_fold_test) 
        else:
            y_proba = model.predict_proba(X_fold_test)
            y_scores = y_proba[:,1]

        auc = roc_auc_score(y_fold_test, y_scores)

        results['AUC_fold_'+str(count)] = auc
        results['fold_idxs'] = (train_idx, test_idx) #recording train and test indexes as reference
        preds+=list(y_scores)
        tests+=list(y_fold_test)
    
    fpr, tpr, thresholds = roc_curve(tests, preds) #false positive rates, true positive rates and thresholds
    auc = roc_auc_score(tests, preds)
    print("AUC {:.3f}".format(auc))

    results['fpr'] = fpr
    results['tpr'] = tpr
    results['auc'] = auc

    return results



#-----------------------------------------
# TESTING METHODS
#-----------------------------------------

def model_oostest(model, X_test, y_test):
    """
    This function tests the model performance on out of sample data
    """

    #cm
    y_score = model.predict(X_test)
    cm = confusion_matrix(y_test, y_score)
    rcm = cm_ratio(cm)

    #metrics
    y_scores = model.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, y_scores)
    auc = roc_auc_score(y_test, y_scores)

    print("AUC {:.3f}".format(auc))

    return {'fpr':fpr, 'tpr':tpr, 'auc':auc, 'rcm':rcm, 'cm':cm}


#-----------------------------------------
# MODEL DIAGNOSTICS UTILS
#-----------------------------------------


def cm_ratio(cm):
    """
    This function takes as an input a scikit learn confusion matrix and returns it with ratio values.
    """

    rcm = np.empty([2,2])
    rcm[0, :] = cm[0, :] / float(sum(cm[0, :]))
    rcm[1, :] = cm[1, :] / float(sum(cm[0, :]))
        
    print("Confusion matrix: \n" + np.array_str(rcm, precision=5, suppress_small=True))

    return rcm


def get_features_importances(trained_model, feature_labels):
    """
    This functions, given a trained model and the labels of the features used to train it,
    will return a dictionary with the weights (for linear models) or the importance (for tree models)
    for every feature
    """

    feat_imp = {}

    if hasattr(trained_model, "feature_importances_"):
        importances = trained_model.feature_importances_
    
    elif hasattr(trained_model, "coef_"):
        importances = trained_model.coef_[0]

    else:
        print("None of the weights/importance module found...")
        return feat_imp

    feat_imp = dict(zip(feature_labels, importances))
    return feat_imp

#-----------------------------------------
# DATA AND MODEL SAVING UTILS
#-----------------------------------------


def encode_name(model_name, prefix):
    """
    This function create a unique name for each model based on datetime.
    It will output a name in the format: prefix_modelname_yearmonthday_hourminsec
    """

    year = str(datetime.datetime.now().year)[2:]
    month = str(datetime.datetime.now().month)
    if len(month)==1:
        month = '0'+month
    day = str(datetime.datetime.now().day)
    if len(day)==1:
        day = '0'+day

    hour = str(datetime.datetime.now().hour)
    min = str(datetime.datetime.now().minute)
    sec = str(datetime.datetime.now().second)

    postfix = '_'+year+month+day+'_'+hour+min+sec
    filename = prefix +'_'+ model_name + postfix+'.pkl'

    return filename


def save_sk_model(model, datafolder, model_name, prefix):
    """
    This function saves a scikit learn model in pickle format
    """

    filename = encode_name(model_name, prefix) #creating file name

    filepath = datafolder+filename

    # Create target folder if it doesn't exist
    if not os.path.exists(datafolder):
        os.mkdir(datafolder)

    print("- Saving model to {}".format(filepath))
    with open(filepath, "wb") as pickle_file:
            pickle.dump(model, pickle_file)

    return (filename, filepath)


def save_dictionary(dict, datafolder, dict_name, postfix):
    """
    This function saves a dictionary to a pickle file
    """
    filepath = datafolder+dict_name+postfix+'.pkl'

    # Create target folder if it doesn't exist
    if not os.path.exists(datafolder):
        os.mkdir(datafolder)

    print("- Saving dictionary to {}".format(filepath))
    with open(filepath, "wb") as pickle_file:
            pickle.dump(dict, pickle_file)
    return filepath
   
#-----------------------------------------
# EXPERIMENT TRACKING
#-----------------------------------------

def mlf_sk_tracking(experiment_name, prefix, postfix, modeltype, trainfile, testfile, datafolder,
                    train_size, test_size, model, model_kfold, model_oos, timeSeqValid, feat_imp_dict,
                    CrossValFolds=None, save_model=False, filename=None, filepath=None,
                    save_results_for_viz=False, dict_name=None):
    """
    This function is an auxiliar function of models_loop which activates the mlflow tracking of an experiment
    using RandomForestClassifier and/or SGDClassifier implemented in scikit-learn.
    Inputs required are:
    - experiment_name: the name of the experiment
    - prefix: the prefix used for the experiment for files sourcing and saving
    - postfix: the postfix used for the experiment for files sourcing and saving
    - modeltype: the type of model used
    - trainfile: the name of the training files
    - testfile: the name of the testing files
    - datafolder: the folder where preprocessed data of the experiment have been saved
    - train_size: the size of the training set
    - test_size: the size of the test set
    - model: the model used during the experiment
    - model_kfold: the output of the model_diag or model_diag_seq used during the experiment (dict)
    - model_oos: the output of out of sample testing used during the experiment (dict)
    - timeSeqValid: boolean value indicating if sequential validation has been used
    - feat_imp_dict: a dictionary containing the feature importances
    - save_model: boolean indicating if the model has been stored somewhere
    - filename: the name of the model
    - filepath: the path where the model has been saved
    """

    #checking the name of existing experiments
    expnames = set([exp.name for exp in mlflow.tracking.MlflowClient().list_experiments()])

    #creating a new experiment if its name is not among the existing ones
    if experiment_name not in expnames:
        print("- Creating the new experiment '{}',  the following results will be saved in it...".format(experiment_name))
        exp_id = mlflow.create_experiment(experiment_name)
    else: #adding new results to the existing one otherwise
        print("- Activating existing experiment '{}', the following results will be saved in it...".format(experiment_name))
        mlflow.set_experiment(experiment_name)
        exp_id = mlflow.tracking.MlflowClient().get_experiment_by_name(experiment_name).experiment_id

    with mlflow.start_run(experiment_id=exp_id, run_name=prefix + modeltype): #create and initialize experiment

        print("- Tracking the experiment on mlflow...")

        #experiment type tracking
        mlflow.log_param("experiment_type", prefix)

        trainfilepath = datafolder + prefix + trainfile + postfix+'.pkl'
        testfilepath = datafolder + prefix + testfile + postfix+'.pkl'

        #source file tracking
        mlflow.log_param("train_file_path", trainfilepath)
        mlflow.log_param("train_file_name", prefix + trainfile + postfix+'.pkl')
        mlflow.log_param("test_file_path", testfilepath)
        mlflow.log_param("test_file_name", prefix + testfile + postfix+'.pkl')
        mlflow.log_param("train_size", train_size)
        mlflow.log_param("test_size", test_size)

        #model info and hyperparameters tracking
        mlflow.log_param("model_type", modeltype)

        if save_model:
            if filename!=None or filepath!=None:
                mlflow.log_param("model_filename", filename)
                mlflow.log_param("model_filepath", filepath)
            else:
                print("WARNING! - filename and filepath set to 'None'! Please change them to a meaningful output path. The models output path have not been saved in mlflow!")
        else:
            mlflow.log_param("model_filepath", None)

        mlflow.log_param("model_filename", None)

        hpar = model.get_params()
        for par_name in hpar.keys():
            mlflow.log_param(par_name, hpar[par_name])

        #kfold validation metrics tracking
        auc_kf_general = round(model_kfold['auc'],3)

        if not timeSeqValid:
            mlflow.log_metric("validation_nfolds", CrossValFolds)

            for fold in range(1,CrossValFolds+1):
                auc_kf_fold = round(model_kfold['AUC_fold_'+str(fold)],3)
                mlflow.log_metric("val_auc_fold_"+str(fold), auc_kf_fold)
        else:
            count=0
            for fold in model_kfold.keys():
                count+=1
                if 'AUC_fold_' in fold:
                    auc_seq_fold = round(model_kfold[fold],3)
                    mlflow.log_metric("val_auc_fold_"+str(count), auc_seq_fold)
                    mlflow.log_metric("validation_nfolds", count)

        mlflow.log_metric("val_auc", auc_kf_general)

        #test metrics tracking
        auc = round(model_oos['auc'],3)
        rcm = model_oos['rcm']

        tnr, fpr, fnr, tpr = rcm.ravel()

        mlflow.log_metric("test_auc", auc)
        mlflow.log_metric("test_tpr", round(tpr,4)) #true positive rate
        mlflow.log_metric("test_fpr", round(fpr,4)) #false positive rate
        mlflow.log_metric("test_fnr", round(fnr,4)) #false negative ratee
        mlflow.log_metric("test_tnr", round(tnr,4)) #true negative rate

        cm = model_oos['cm']
        tn, fp, fn, tp = cm.ravel()
        mlflow.log_metric("test_tp", tp) #true positive rate
        mlflow.log_metric("test_fp", fp) #false positive rate
        mlflow.log_metric("test_fn", fn) #false negative ratee
        mlflow.log_metric("test_tn", tn) #true negative rate


        #features importances tracking
        for f in feat_imp_dict.keys():
            mlflow.log_metric("f_"+f, feat_imp_dict[f])

        #MODEL STORAGE
        #storing the model file as pickle
        mlflow.sklearn.log_model(model, "model")

        #ARTIFACTS STORAGE
        #storing pipeline-processed trainset and testset
        mlflow.log_artifact(trainfilepath, "train_file")
        mlflow.log_artifact(testfilepath, "test_file")

        if save_results_for_viz:
            if dict_name!=None:
                mlflow.log_artifact(dict_name, "results_dict_for_viz")
            else:
                print("WARNING! - dictiname set to 'None'! Please change it to a meaningful output path. The dict viz output has not been saved in mlflow as artifact!")

        print("- Experiment tracked.")

#-----------------------------------------
# MAIN
#-----------------------------------------

def main():
    print("models_utils.py executed/loaded..")

if __name__ == "__main__":
    main()